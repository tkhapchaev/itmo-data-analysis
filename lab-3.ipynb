{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:04:44.621059Z",
     "start_time": "2025-03-08T15:04:44.617277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import requests\n",
    "import feedparser\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "PROXY_API_KEY = \"$$\""
   ],
   "id": "5f73410e6d748541",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Timur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Извлечение публикаций с arXiv\n",
    "Функция fetch_arxiv делает запрос к arXiv API по заданной теме и возвращает список публикаций с заголовками, авторами и аннотациями"
   ],
   "id": "1bc99994b20fe902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:05:12.682068Z",
     "start_time": "2025-03-08T15:05:06.432660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_arxiv(query, max_results=250):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": \"submittedDate\",\n",
    "        \"sortOrder\": \"descending\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    feed = feedparser.parse(response.text)\n",
    "    publications = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        pub = {\n",
    "            \"title\": entry.title.strip(),\n",
    "            \"authors\": [author.name for author in entry.authors],\n",
    "            \"summary\": entry.summary.strip()\n",
    "        }\n",
    "        publications.append(pub)\n",
    "\n",
    "    return publications\n",
    "\n",
    "query = \"machine learning\"\n",
    "publications = fetch_arxiv(query, max_results=1000)\n",
    "print(f\"Получено {len(publications)} публикаций\")"
   ],
   "id": "7834b3a7eba5951b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Получено 1000 публикаций\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Разбиение аннотаций на чанки\n",
    "Функция chunk_text делит текст аннотаций на чанки по заданному количеству слов с параметризованным перекрытием"
   ],
   "id": "30b2f992916945ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:05:25.157778Z",
     "start_time": "2025-03-08T15:05:24.601261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_average_summary_len():\n",
    "    words_summary = 0\n",
    "\n",
    "    for pub in publications:\n",
    "        words_summary += len(word_tokenize(pub[\"summary\"]))\n",
    "\n",
    "    print(f\"Среднее количество слов в аннотации публикации: {math.floor(words_summary/len(publications))}\\nВсего публикаций: {len(publications)}\")\n",
    "\n",
    "find_average_summary_len()"
   ],
   "id": "62f5234c0dedf215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее количество слов в аннотации публикации: 203\n",
      "Всего публикаций: 1000\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:05:40.316103Z",
     "start_time": "2025-03-08T15:05:39.747416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        chunk = words[start:start+chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "corpus_chunks = []\n",
    "\n",
    "for pub in publications:\n",
    "    chunks = chunk_text(pub[\"summary\"], chunk_size=100, overlap=25)\n",
    "    corpus_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Общее количество чанков: {len(corpus_chunks)}\")"
   ],
   "id": "9606b1e7f128ad32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество чанков: 3197\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Векторизация текста и построение FAISS индекса\n",
    "Используем модель 'all-MiniLM-L6-v2' для получения эмбеддингов и создаем FAISS индекс для быстрого поиска"
   ],
   "id": "8a9cee6d06e8dae4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:06:19.200264Z",
     "start_time": "2025-03-08T15:05:52.361481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def build_faiss_index(chunks, model):\n",
    "    embeddings = model.encode(chunks)\n",
    "    embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    return index, embeddings_np\n",
    "\n",
    "index, embeddings_np = build_faiss_index(corpus_chunks, embedding_model)\n",
    "print(f\"Построен FAISS индекс с {index.ntotal} векторами\")"
   ],
   "id": "e0f77fb3244bde60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построен FAISS индекс с 3197 векторами\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Интеграция с LLM для извлечения ключевых идей\n",
    "Настраиваем связь с LLM через Proxy API. Функция extract_key_ideas получает ключевые идеи из текста"
   ],
   "id": "f19bbe76137d76f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:57:11.876779Z",
     "start_time": "2025-03-08T15:57:07.642745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = openai.Client(\n",
    "    api_key=PROXY_API_KEY,\n",
    "    base_url=\"https://api.proxyapi.ru/openai/v1\",\n",
    ")\n",
    "\n",
    "def isNullOrWhiteSpace(str=None):\n",
    "    return not str or str.isspace()\n",
    "\n",
    "def extract_key_ideas(text, query=\"\", model_name=\"gpt-4o\"):\n",
    "    if len(query) > 0 and not(isNullOrWhiteSpace(query)):\n",
    "        prompt = f\"Используя данный тебе текст и пользовательский запрос, извлеки из текста ключевые идеи и основные результаты (по пунктам). Затем, опираясь на этот текст, дай ответ на запрос пользователя (в отдельном абзаце). Если запрос некорректный, то ответ на него пропусти, не предупреждая об этом. На русском языке. Запрос: {query}\\nТекст:\\n\\n{text}\"\n",
    "    else:\n",
    "        prompt = f\"Используя данный тебе текст, извлеки из него ключевые идеи и основные результаты (по пунктам, на русском языке):\\n\\n{text}\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "sample_chunk_index = 0\n",
    "sample_chunk = corpus_chunks[sample_chunk_index]\n",
    "key_ideas_sample = extract_key_ideas(sample_chunk)\n",
    "\n",
    "print(key_ideas_sample)"
   ],
   "id": "c50b7ec53d34334f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Разработан масштабный закон взаимной информации для двудольных графов в контексте естественного языка, который описывает дальнобойные зависимости.\n",
      "2. Доказано, что этот масштабный закон отличается от традиционной взаимной информации для двух точек и масштабируется независимо от неё.\n",
      "3. Этот закон является ключом к пониманию моделирования языка с длинным контекстом.\n",
      "4. На основе данного масштабного закона сформулировано условие моделирования языка с длинным контекстом (L²M), которое связывает способность модели эффективно работать с длинным контекстом с масштабом размера её скрытых состояний для хранения информации о прошлом.\n",
      "5. Результаты были подтверждены экспериментами на трансформерных моделях.\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Поиск по корпусу с обработкой опечаток\n",
    "Используем библиотеку SpellChecker для автоматической коррекции опечаток в запросе.\n",
    "Функция search_corpus корректирует запрос, векторизует его и ищет топ-k ближайших чанков в FAISS индексе"
   ],
   "id": "508c05f5b543875e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T15:57:26.204846Z",
     "start_time": "2025-03-08T15:57:16.603970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "def correct_query(query):\n",
    "    tokens = word_tokenize(query)\n",
    "    corrected_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            corrected = spell.correction(token)\n",
    "            corrected_tokens.append(corrected)\n",
    "        else:\n",
    "            corrected_tokens.append(token)\n",
    "\n",
    "    corrected_query = \" \".join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def search_corpus(query, index, corpus_chunks, model, k=5):\n",
    "    query_corrected = correct_query(query)\n",
    "    print(f\"Запрос пользователя: {query_corrected}\\n\")\n",
    "\n",
    "    query_embedding = model.encode([query_corrected]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [corpus_chunks[i] for i in indices[0] if i < len(corpus_chunks)]\n",
    "\n",
    "    return results\n",
    "\n",
    "# user_query = input()\n",
    "user_query = \"is mashine learninn the technkligy of the futyre?\"\n",
    "\n",
    "relevant_chunks = search_corpus(user_query, index, corpus_chunks, embedding_model, k=5)\n",
    "combined_text = \"\\n\\n\".join(relevant_chunks)\n",
    "key_ideas_result = extract_key_ideas(combined_text, correct_query(user_query))\n",
    "\n",
    "print(key_ideas_result)"
   ],
   "id": "c0887ef398178ba3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос пользователя: is machine learning the technology of the future ?\n",
      "\n",
      "Ключевые идеи и основные результаты:\n",
      "\n",
      "1. Возрастающее количество соединений в Интернете вещей (IoT) способствует формированию \"интеллектуальных предприятий\", которые используют модели, основанные на машинном обучении, для извлечения полезной информации из данных.\n",
      "2. Проблемы эффективности и конфиденциальности в традиционных моделях машинного обучения привели к возникновению парадигмы федеративного обучения (FL), которая позволяет нескольким предприятиям совместно обучать модель.\n",
      "3. Модели, обученные с помощью FL, обычно демонстрируют худшую производительность по сравнению с централизованными моделями, особенно когда данные обучения предприятий не являются независимыми и идентично распределенными.\n",
      "4. В статье представлен обзор приложений моделей-основ в науке об окружающей среде, включая предсказание, генерацию данных, ассимиляцию данных, даунскейлинг, ансамблирование моделей и принятие решений.\n",
      "5. Описан процесс разработки моделей, включая сбор данных, дизайн архитектуры, обучение, настройку и оценку.\n",
      "6. Подчеркивается важность междисциплинарного подхода для полного использования потенциала машинного обучения в биомеханике ходьбы и спорта.\n",
      "\n",
      "Ответ на пользовательский запрос:\n",
      "\n",
      "Машинное обучение играет значительную роль в создании устойчивых решений в науке об окружающей среде и интеллектуальных предприятиях, что подчёркивает его значимость в будущем развитии технологий. Хотя есть вызовы, такие как модели федеративного обучения, которые пока уступают централизованным моделям, непрерывное развитие и внедрение междисциплинарных подходов говорят о том, что машинное обучение является ключевой технологией будущего.\n"
     ]
    }
   ],
   "execution_count": 120
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
